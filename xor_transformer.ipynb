{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpy4/v+KxN+i8d9srnni4Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/foxtrotmike/CS909/blob/master/xor_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Self-Attention to Solve the 2D XOR Problem in PyTorch\n",
        "\n",
        "By Fayyaz Minhas.\n",
        "\n",
        "In this tutorial, we will explore an innovative approach to solving the 2D XOR problem using a self-attention mechanism in PyTorch. The XOR problem is a classic example used to demonstrate the limitations of linear models and the power of more complex architectures.\n",
        "\n",
        "## Overall Approach\n",
        "The key idea is to model each bit in the input as a token and use an attention mechanism to develop a feature representation for these tokens. This allows the model to dynamically weigh the importance of each bit in the input sequence, leading to a more robust representation that can solve the XOR problem effectively.\n",
        "\n",
        "## Self-Attention Layer\n",
        "The self-attention mechanism enables the model to focus on different parts of the input sequence by computing a weighted sum of the value vectors, where the weights are determined by the similarity between the query and key vectors."
      ],
      "metadata": {
        "id": "D0ZZtZuJi5Nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_qk = 2, d_model = 2):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.W_q = nn.Linear(1, d_qk, bias=False)\n",
        "        self.W_k = nn.Linear(1, d_qk, bias=False)\n",
        "        self.W_v = nn.Linear(1, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Weighted sum of values\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "\n",
        "        return output, attention_weights\n"
      ],
      "metadata": {
        "id": "ANWrMH53jF6X"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation:\n",
        "\n",
        "Query, Key, and Value Vectors:\n",
        "Each bit in the input is treated as a token and transformed into query (Q), key (K), and value (V) vectors using linear layers.\n",
        "\n",
        " Attention Scores:\n",
        "The attention scores are computed using the dot product of Q and K, scaled by the square root of the model dimension to ensure stable gradients.\n",
        "\n",
        "Attention Weights:\n",
        "The softmax function is applied to the scores to obtain the attention weights, which determine the importance of each token.\n",
        "\n",
        "Weighted Sum:\n",
        "The final output is obtained by computing the weighted sum of the value vectors, where the weights are the attention scores.\n",
        "\n",
        "## XOR Attention Model\n",
        "The model combines the self-attention layer with a fully connected layer to predict the XOR output."
      ],
      "metadata": {
        "id": "NImYMIzBjGg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class XORAttentionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(XORAttentionModel, self).__init__()\n",
        "        d_model = 2\n",
        "        self.attention = SelfAttention(d_model=d_model)\n",
        "        self.fc1 = nn.Linear(2 * d_model, 2)\n",
        "        self.fc2 = nn.Linear(2, 1)\n",
        "        # Freeze the attention layer parameters\n",
        "        for param in self.attention.parameters():\n",
        "            param.requires_grad = True #Can be set to False to check if it is indeed the attention that is making a difference\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, attention_weights = self.attention(x)\n",
        "        attn_output = attn_output.view(attn_output.size(0), -1)\n",
        "        attn_output = F.tanh(attn_output) #Non linearity\n",
        "        fc1_output = self.fc1(attn_output) #just to convert to 2D\n",
        "        output = self.fc2(fc1_output)\n",
        "        return output, attention_weights, fc1_output\n"
      ],
      "metadata": {
        "id": "rGPsY6ChjUkH"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation:\n",
        "Input Tokens:\n",
        "Each bit of the XOR input is modeled as a token and passed through the self-attention layer.\n",
        "\n",
        "Feature Representation:\n",
        "The attention layer produces a feature representation by considering the importance of each bit in the input.\n",
        "\n",
        "Fully Connected Layer:\n",
        "The transformed output from the attention layer is passed through a fully connected layer to produce the final prediction.\n",
        "\n",
        "Training the Model:\n",
        "The model is trained on the XOR dataset using Mean Squared Error (MSE) loss and the Adam optimizer."
      ],
      "metadata": {
        "id": "l5FARfy6jXFw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU9-NNdl8xOS",
        "outputId": "6c264af8-7b9e-42ec-ac32-b3e87fe34f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, Loss: 0.0001357378641841933\n",
            "Epoch 200, Loss: 1.5329132452279737e-07\n",
            "Epoch 300, Loss: 2.0622257579816505e-05\n",
            "Epoch 400, Loss: 1.0135195793736784e-07\n",
            "Epoch 500, Loss: 9.096464737012866e-08\n",
            "Epoch 600, Loss: 0.0028985911048948765\n",
            "Epoch 700, Loss: 1.0409092965346645e-07\n",
            "Epoch 800, Loss: 5.261746593987482e-10\n",
            "Epoch 900, Loss: 0.003003119956701994\n",
            "Epoch 1000, Loss: 8.488447633681062e-08\n",
            "Epoch 1100, Loss: 6.706748933993367e-09\n",
            "Epoch 1200, Loss: 0.0020726402290165424\n",
            "Epoch 1300, Loss: 2.0104394593545294e-07\n",
            "Epoch 1400, Loss: 1.5863045632613648e-07\n",
            "Epoch 1500, Loss: 3.0250492272898555e-05\n",
            "Epoch 1600, Loss: 1.3077320772936218e-06\n",
            "Epoch 1700, Loss: 5.18033857588307e-07\n",
            "Epoch 1800, Loss: 0.01752374693751335\n",
            "Epoch 1900, Loss: 2.225025639290834e-07\n",
            "Epoch 2000, Loss: 1.4997780795056315e-11\n",
            "Output: \n",
            " tensor([[-1.0000],\n",
            "        [ 1.0000],\n",
            "        [ 1.0000],\n",
            "        [-1.0000]])\n",
            "Transformed representations at the output of the attention block:\n",
            "Input: [0.0, 0.0], Transformed Representation: [-0.33158165216445923, -0.4196920692920685]\n",
            "Input: [0.0, 1.0], Transformed Representation: [0.9644567370414734, 0.5007822513580322]\n",
            "Input: [1.0, 0.0], Transformed Representation: [0.9798806309700012, 0.4389570653438568]\n",
            "Input: [1.0, 1.0], Transformed Representation: [-0.2898065447807312, -0.5871206521987915]\n",
            "Attention Weights for each input:\n",
            "Input: [0.0, 0.0], Attention Weights: [[0.5, 0.5], [0.5, 0.5]]\n",
            "Input: [0.0, 1.0], Attention Weights: [[0.5, 0.5], [0.6086821556091309, 0.39131784439086914]]\n",
            "Input: [1.0, 0.0], Attention Weights: [[0.39131784439086914, 0.6086821556091309], [0.5, 0.5]]\n",
            "Input: [1.0, 1.0], Attention Weights: [[0.5, 0.5], [0.5, 0.5]]\n"
          ]
        }
      ],
      "source": [
        "# XOR input and output\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
        "outputs = np.array([[-1], [1], [1], [-1]], dtype=np.float32)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "inputs = torch.tensor(inputs).unsqueeze(-1)\n",
        "outputs = torch.tensor(outputs)\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = XORAttentionModel()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2000):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    preds, _, _ = model(inputs)\n",
        "    loss = criterion(preds, outputs)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "\n",
        "# Evaluate and print attention weights and transformed representations\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs, attention_weights, attn_outputs = model(inputs)\n",
        "print(\"Output: \\n\",outputs)\n",
        "print(\"Transformed representations at the output of the attention block:\")\n",
        "for i, input in enumerate(inputs):\n",
        "    print(f\"Input: {input.squeeze().tolist()}, Transformed Representation: {attn_outputs[i].squeeze().tolist()}\")\n",
        "\n",
        "print(\"Attention Weights for each input:\")\n",
        "for i, input in enumerate(inputs):\n",
        "    print(f\"Input: {input.squeeze().tolist()}, Attention Weights: {attention_weights[i].squeeze().tolist()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions\n",
        "\n",
        "In this tutorial, we demonstrated how to implement a self-attention layer in PyTorch and use it to solve the 2D XOR problem. By modeling each bit in the input as a token and applying the attention mechanism, the model can dynamically weigh the importance of each bit, leading to a robust feature representation that effectively solves the XOR problem. This approach highlights the power of attention mechanisms and their potential applications in more complex tasks and models, such as transformers in natural language processing and beyond.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2h8HJExujrNI"
      }
    }
  ]
}